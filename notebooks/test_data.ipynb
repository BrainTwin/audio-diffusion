{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noising Data to test how FAD is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_waveform(data, noise_level=0.2):\n",
    "    \"\"\" Add random noise to the waveform \"\"\"\n",
    "    noise = np.random.randn(*data.shape) * noise_level\n",
    "    distorted_data = data + noise\n",
    "    return distorted_data\n",
    "\n",
    "def process_audio_files(src_directory, dst_directory):\n",
    "    \"\"\" Process all .wav files in the source directory and save them to the destination directory with added noise \"\"\"\n",
    "    if not os.path.exists(dst_directory):\n",
    "        os.makedirs(dst_directory)\n",
    "    \n",
    "    for filename in tqdm(os.listdir(src_directory)):\n",
    "        if filename.endswith('.wav'):\n",
    "            src_path = os.path.join(src_directory, filename)\n",
    "            dst_path = os.path.join(dst_directory, filename)\n",
    "            \n",
    "            # Read audio file\n",
    "            data, samplerate = sf.read(src_path)\n",
    "            \n",
    "            # Add noise to the audio data\n",
    "            distorted_data = add_noise_to_waveform(data)\n",
    "            \n",
    "            # Save the distorted audio file\n",
    "            sf.write(dst_path, distorted_data, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:18<00:00,  5.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "source_directory = '../cache/spotify_sleep_dataset/waveform_small'\n",
    "destination_directory = '../cache/spotify_sleep_dataset/waveform_small_distorted_2'\n",
    "\n",
    "# Process the audio files\n",
    "process_audio_files(source_directory, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Mel Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "dataset_name = '../cache/spotify_sleep_dataset/waveform_small/mel_spectrogram_256_256'\n",
    "dataset_config_name = None\n",
    "vae = None\n",
    "\n",
    "dataset = load_from_disk(\n",
    "    dataset_name,\n",
    "    storage_options=dataset_config_name)[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Determine image resolution\n",
    "    resolution = dataset[0][\"image\"].height, dataset[0][\"image\"].width\n",
    "\n",
    "    augmentations = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "\n",
    "    def transforms(examples):\n",
    "        if vae is not None and vqvae.config[\"in_channels\"] == 3:\n",
    "            images = [\n",
    "                augmentations(image.convert(\"RGB\"))\n",
    "                for image in examples[\"image\"]\n",
    "            ]\n",
    "        else:\n",
    "            images = [augmentations(image) for image in examples[\"image\"]]\n",
    "        if encodings is not None:\n",
    "            encoding = [encodings[file] for file in examples[\"audio_file\"]]\n",
    "            return {\"input\": images, \"encoding\": encoding}\n",
    "        return {\"input\": images}\n",
    "\n",
    "    dataset.set_transform(transforms)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    if encodings is not None:\n",
    "        encodings = pickle.load(open(args.encodings, \"rb\"))\n",
    "\n",
    "\n",
    "    if args.from_pretrained is not None:\n",
    "        pipeline = AudioDiffusionPipeline.from_pretrained(args.from_pretrained)\n",
    "        mel = pipeline.mel\n",
    "        model = pipeline.unet\n",
    "        if hasattr(pipeline, \"vqvae\"):\n",
    "            vqvae = pipeline.vqvae\n",
    "\n",
    "    else:\n",
    "        if args.encodings is None:\n",
    "            model = UNet2DModel(\n",
    "                sample_size=resolution if vqvae is None else latent_resolution,\n",
    "                in_channels=1\n",
    "                if vqvae is None else vqvae.config[\"latent_channels\"],\n",
    "                out_channels=1\n",
    "                if vqvae is None else vqvae.config[\"latent_channels\"],\n",
    "                layers_per_block=2,\n",
    "                block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "                down_block_types=(\n",
    "                    \"DownBlock2D\",\n",
    "                    \"DownBlock2D\",\n",
    "                    \"DownBlock2D\",\n",
    "                    \"DownBlock2D\",\n",
    "                    \"AttnDownBlock2D\",\n",
    "                    \"DownBlock2D\",\n",
    "                ),\n",
    "                up_block_types=(\n",
    "                    \"UpBlock2D\",\n",
    "                    \"AttnUpBlock2D\",\n",
    "                    \"UpBlock2D\",\n",
    "                    \"UpBlock2D\",\n",
    "                    \"UpBlock2D\",\n",
    "                    \"UpBlock2D\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            model = UNet2DConditionModel(\n",
    "                sample_size=resolution if vqvae is None else latent_resolution,\n",
    "                in_channels=1\n",
    "                if vqvae is None else vqvae.config[\"latent_channels\"],\n",
    "                out_channels=1\n",
    "                if vqvae is None else vqvae.config[\"latent_channels\"],\n",
    "                layers_per_block=2,\n",
    "                block_out_channels=(128, 256, 512, 512),\n",
    "                down_block_types=(\n",
    "                    \"CrossAttnDownBlock2D\",\n",
    "                    \"CrossAttnDownBlock2D\",\n",
    "                    \"CrossAttnDownBlock2D\",\n",
    "                    \"DownBlock2D\",\n",
    "                ),\n",
    "                up_block_types=(\n",
    "                    \"UpBlock2D\",\n",
    "                    \"CrossAttnUpBlock2D\",\n",
    "                    \"CrossAttnUpBlock2D\",\n",
    "                    \"CrossAttnUpBlock2D\",\n",
    "                ),\n",
    "                cross_attention_dim=list(encodings.values())[0].shape[-1],\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiodiff_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
