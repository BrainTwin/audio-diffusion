=========================================
TESTING SMALL DATASET WITH NOHUP

nohup accelerate launch --config_file config/accelerate_local.yaml \
scripts/train_unet.py \
--dataset_name cache/spotify_sleep_dataset/waveform_small/mel_spectrogram \
--hop_length 256 \
--n_fft 1024 \
--output_dir models/ssd_light_training_waveform_small_64_64 \
--train_batch_size 4 \
--eval_batch_size 8 \
--num_epochs 8 \
--gradient_accumulation_steps 4 \
--learning_rate 1e-4 \
--lr_warmup_steps 500 \
--mixed_precision no \
--save_model_epochs 2 \
--save_images_epochs 2 \
--num_train_steps 1000 \
--num_inference_steps 1000 \
--train_scheduler ddpm \
--test_scheduler ddpm \
> logs/ssd_light_training_waveform_small_64_64 2>&1 &

=========================================
GENERATE MEL SPECTROGRAMS FROM AUDIO AND SAVE TO File

python scripts/audio_to_images.py \
--resolution 64 \
--hop_length 256 \
--n_fft 1024 \
--input_dir cache/musiccaps/waveform \
--output_dir cache/musiccaps/waveform/mel_spec_64_64


=========================================
GENERATE SAMPLE IMAGES FROM TRAINED MODEL

python scripts/inference_unet.py \
--model_path path/to/model \
--num_images 512 \
--num_inference_steps 50 \
--eval_batch_size 64 \
--scheduler ddim

=========================================
COMPUTE FAD SCORE BETWEEN REFERENCE AND GENERATED SET

fadtk clap-laion-audio /path/to/baseline/audio /path/to/evaluation/audio

=========================================
TRAIN VAE

nohup python scripts/train_vae.py \
    --dataset_name cache/spotify_sleep_dataset/waveform_small/mel_spec_64_64 \
    --batch_size 8 \
    --gradient_accumulation_steps 2 \
    --save_images_batches 4 \
    --max_epochs 200 > logs/train_vae_ssd_64 2>&1 &


=========================================


=========================================


=========================================


=========================================


write me a python script which will loop over certain variables for the following command:

accelerate launch --config_file config/accelerate_local.yaml \
scripts/train_unet.py \
--dataset_name cache/spotify_sleep_dataset/waveform_small/mel_spectrogram \
--hop_length 256 \
--n_fft 1024 \
--output_dir models/ssd_light_training_waveform_small_64_64 \
--train_batch_size 32 \
--eval_batch_size 8 \
--num_epochs 1000 \
--gradient_accumulation_steps 4 \
--learning_rate 1e-4 \
--lr_warmup_steps 500 \
--mixed_precision no \
--save_model_epochs 2 \
--save_images_epochs 2 \
--num_train_steps 1000 \
--num_inference_steps 1000 \
--train_scheduler ddpm \
--test_scheduler ddpm \

In particular, I want to cycle through these variable combinations

dataset_name: /home/th716/rds/hpc-work/audio-diffusion/cache/musiccaps/mel_spec_64_64
output_dir: /home/th716/rds/hpc-work/audio-diffusion/models/musiccaps_64_64

dataset_name: /home/th716/rds/hpc-work/audio-diffusion/cache/drum_samples/mel_spec_64_64
output_dir: /home/th716/rds/hpc-work/audio-diffusion/models/drum_samples_64_64

dataset_name: /home/th716/rds/hpc-work/audio-diffusion/cache/spotify_sleep_dataset/mel_spec_64_64
output_dir: /home/th716/rds/hpc-work/audio-diffusion/models/spotify_sleep_dataset_64_64



dataset_name: /home/th716/rds/hpc-work/audio-diffusion/cache/musiccaps/mel_spec_256_256
output_dir: /home/th716/rds/hpc-work/audio-diffusion/models/musiccaps_256_256

dataset_name: /home/th716/rds/hpc-work/audio-diffusion/cache/drum_samples/mel_spec_256_256
output_dir: /home/th716/rds/hpc-work/audio-diffusion/models/drum_samples_256_256

dataset_name: /home/th716/rds/hpc-work/audio-diffusion/cache/spotify_sleep_dataset/mel_spec_256_256
output_dir: /home/th716/rds/hpc-work/audio-diffusion/models/spotify_sleep_dataset_256_256