#!/bin/bash
#SBATCH -J inference_final_ssd_2048_128_bigvgan_so_ms203706
#SBATCH -A NALLAPERUMA-SL2-GPU
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --gres=gpu:1
#SBATCH --time=16:00:00
#SBATCH --mail-type=ALL
#SBATCH --output=experiments/final_hpc_runs/inference_final_ssd_2048_128_bigvgan_so_ms203706.out
#SBATCH -p ampere

. /etc/profile.d/modules.sh
module purge
module load rhel8/default-amp
module load miniconda/3

source ~/.bashrc
conda init bash
conda activate audiodiff_env

application="accelerate"
options="launch --config_file /home/th716/rds/hpc-work/audio-diffusion/config/accelerate_local.yaml \
/home/th716/rds/hpc-work/audio-diffusion/scripts/inference_unet.py \
--pretrained_model_path /home/th716/rds/hpc-work/audio-diffusion/models/final_models/final_ssd_2048_128_bigvgan_so/model_step_203706 \
--num_images 1024 \
--eval_batch_size 16 \
--num_inference_steps 1000 \
--scheduler ddpm \
--n_iter 32 \

--mel_spec_method bigvgan"

CMD="$application $options"
workdir="/home/th716/rds/hpc-work/audio-diffusion"

cd $workdir
echo "Changed directory to `pwd`."
echo "JobID: $SLURM_JOB_ID"
echo "Time: `date`"
echo "Running on master node: `hostname`"
echo "Current directory: `pwd`"

if [ "$SLURM_JOB_NODELIST" ]; then
    export NODEFILE=`generate_pbs_nodefile`
    cat $NODEFILE | uniq > machine.file.$SLURM_JOB_ID
    echo "Nodes allocated:"
    echo `cat machine.file.$SLURM_JOB_ID | sed -e 's/\..*$//g'`
fi

echo "Executing command:"
echo "$CMD"
eval $CMD
